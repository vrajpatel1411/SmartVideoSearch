{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project\n",
    "\n",
    "In this Project, you will bring together many of the tools and techniques that you have learned throughout this course into a final project. You can choose from many different paths to get to the solution. \n",
    "\n",
    "### Business scenario\n",
    "\n",
    "You work for a training organization that recently developed an introductory course about machine learning (ML). The course includes more than 40 videos that cover a broad range of ML topics. You have been asked to create an application that will students can use to quickly locate and view video content by searching for topics and key phrases.\n",
    "\n",
    "You have downloaded all of the videos to an Amazon Simple Storage Service (Amazon S3) bucket. Your assignment is to produce a dashboard that meets your supervisorâ€™s requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project steps\n",
    "\n",
    "To complete this project, you will follow these steps:\n",
    "\n",
    "1. [Viewing the video files](#1.-Viewing-the-video-files)\n",
    "2. [Transcribing the videos](#2.-Transcribing-the-videos)\n",
    "3. [Normalizing the text](#3.-Normalizing-the-text)\n",
    "4. [Extracting key phrases and topics](#4.-Extracting-key-phrases-and-topics)\n",
    "5. [Creating the dashboard](#5.-Creating-the-dashboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful information\n",
    "\n",
    "The following cell contains some information that might be useful as you complete this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"c56161a939430l3396553t1w744137092661-labbucket-rn642jaq01e9\"\n",
    "job_data_access_role = 'arn:aws:iam::744137092661:role/service-role/c56161a939430l3396553t1w7-ComprehendDataAccessRole-1P24MSS91ADHP'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Viewing the video files\n",
    "([Go to top](#Capstone-8:-Bringing-It-All-Together))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source video files are located in the following shared Amazon Simple Storage Service (Amazon S3) bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('videos'):\n",
    "    os.mkdir(\"videos\")\n",
    "!aws s3 cp s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/ videos --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transcribing the videos\n",
    " ([Go to top](#Capstone-8:-Bringing-It-All-Together))\n",
    "\n",
    "Use this section to implement your solution to transcribe the videos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: moviepy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from moviepy) (1.22.3)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from moviepy) (4.64.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from moviepy) (2.28.1)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from moviepy) (0.4.8)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from moviepy) (2.24.0)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.8)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: speechrecognition in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (3.10.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from speechrecognition) (2.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.26.0->speechrecognition) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.26.0->speechrecognition) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.26.0->speechrecognition) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.26.0->speechrecognition) (3.4)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pydub in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.25.1)\n",
      "46\n",
      "CPU times: user 123 ms, sys: 120 ms, total: 243 ms\n",
      "Wall time: 8.92 s\n"
     ]
    }
   ],
   "source": [
    " %%time\n",
    "# Write your answer/code here\n",
    "!pip install moviepy\n",
    "!pip install speechrecognition\n",
    "!pip install pydub\n",
    "import moviepy.editor as mp\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import make_chunks\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "# I am creating few folder in which I stored original videos, audio, chunked files and finally text files.\n",
    "\n",
    "if not os.path.exists('chunked'):\n",
    "    os.mkdir('chunked')\n",
    "if not os.path.exists('Wav File'):\n",
    "    os.mkdir('Wav File')\n",
    "if not os.path.exists('Transcribe Files'):\n",
    "    os.mkdir('Transcribe Files')\n",
    "    \n",
    "    \n",
    "\n",
    "video_files = [file for file in os.listdir('videos') if file.endswith('.mp4')]\n",
    "\n",
    "print(len(video_files))\n",
    "# extract_text(video_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(fileNames):\n",
    "    i=0\n",
    "    for file in fileNames:\n",
    "        print(i)\n",
    "#         Creating AUDIO FILE FROM THE VIDEO FILE\n",
    "        video=mp.VideoFileClip(f'videos/{file}')\n",
    "        audio_filename= file[:-4]+'.wav'\n",
    "        video.audio.write_audiofile(f'Wav File/{audio_filename}')\n",
    "#         CREATING AUDIO Chunks because the speech recognition library accept small size of audio and we have big audio size.\n",
    "        myaudio=AudioSegment.from_wav(f'Wav File/{audio_filename}')\n",
    "        chunks_length_ms=70000\n",
    "        chunks=make_chunks(myaudio,chunks_length_ms)\n",
    "        text=\"\"\n",
    "#         Iterating through each chunks and extracting text and combining the text and then finally writing that text to the text file\n",
    "        for j,chunk in enumerate(chunks):\n",
    "            chunkName=f\"{file[:-4]}_{j}.wav\"\n",
    "            chunk.export(f\"chunked/{chunkName}\",format=\"wav\")\n",
    "            r=sr.Recognizer()\n",
    "            with sr.AudioFile(f\"chunked/{chunkName}\") as source:\n",
    "                audio_data=r.record(source)\n",
    "                try:\n",
    "                    text =text+\" \"+r.recognize_google(audio_data)\n",
    "                except sr.UnknownValueError:\n",
    "                    print(\"i don't recognized the error\")\n",
    "            os.remove(f\"chunked/{chunkName}\")\n",
    "        filename=file[:-4]+\".txt\"\n",
    "        with open(f'Transcribe Files/{filename}','w') as writer:\n",
    "            writer.write(text)\n",
    "        print(i)\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.95 ms, sys: 9 Âµs, total: 7.96 ms\n",
      "Wall time: 294 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Getting all text from each text file and creating a dataframe which we will use further.\n",
    "import pandas as pd\n",
    "text_files = [file for file in os.listdir(\"Transcribe Files\") if file.endswith('.txt')]\n",
    "def get_data_from_text(text_files):\n",
    "    final_dataset=list()\n",
    "    for i in text_files:\n",
    "        temp=list()\n",
    "        name=i[:-4]\n",
    "        temp.append(name)\n",
    "        with open(f'Transcribe Files/{i}') as f:\n",
    "            contents = f.read()\n",
    "            temp.append(contents)\n",
    "        final_dataset.append(temp)\n",
    "    return final_dataset\n",
    "dataset=get_data_from_text(text_files)\n",
    "final_dataset=pd.DataFrame(dataset,columns=['name','Transcribe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>Transcribe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mod02_Sect03</td>\n",
       "      <td>hi and welcome back this is Section 3 and we'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mod03_Sect02_part2</td>\n",
       "      <td>hi welcome back we'll continue exploring data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mod05_WrapUp_ver2</td>\n",
       "      <td>it's now time to summarize some of the main p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mod03_Sect01</td>\n",
       "      <td>hi and welcome back to module 3 this is Secti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mod05_Sect03_part1</td>\n",
       "      <td>in this section we'll look at preparing custo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mod05_Sect02_part1_ver2</td>\n",
       "      <td>welcome back in this section we'll explore im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mod03_Sect03_part3</td>\n",
       "      <td>hi welcome back now will review how to find c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mod03_Sect04_part2</td>\n",
       "      <td>hi welcome back we'll continue exploring feat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mod03_Sect07_part3</td>\n",
       "      <td>hi welcome back we'll continue exploring how ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mod02_Sect01</td>\n",
       "      <td>hi and welcome to section 1 in this section w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name                                         Transcribe\n",
       "0             Mod02_Sect03   hi and welcome back this is Section 3 and we'...\n",
       "1       Mod03_Sect02_part2   hi welcome back we'll continue exploring data...\n",
       "2        Mod05_WrapUp_ver2   it's now time to summarize some of the main p...\n",
       "3             Mod03_Sect01   hi and welcome back to module 3 this is Secti...\n",
       "4       Mod05_Sect03_part1   in this section we'll look at preparing custo...\n",
       "5  Mod05_Sect02_part1_ver2   welcome back in this section we'll explore im...\n",
       "6       Mod03_Sect03_part3   hi welcome back now will review how to find c...\n",
       "7       Mod03_Sect04_part2   hi welcome back we'll continue exploring feat...\n",
       "8       Mod03_Sect07_part3   hi welcome back we'll continue exploring how ...\n",
       "9             Mod02_Sect01   hi and welcome to section 1 in this section w..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalizing the text\n",
    "([Go to top](#Capstone-8:-Bringing-It-All-Together))\n",
    "\n",
    "Use this section to perform any text normalization steps that are necessary for your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "stopWords = stopwords.words('english')\n",
    "nltk.download('punkt')\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "# In this part I am doing normalization of my text by lower the text, removing extra space, removing stopwords and lematizing the text.\n",
    "def normalized_text(text:str):\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = re.sub('[^\\w\\s]','', text)\n",
    "    text = text.split()\n",
    "    text = [word for word in text if word.isalpha()]\n",
    "    text = [word for word in text if word not in stopWords and len(word) >= 2]\n",
    "    text = [LEMMATIZER.lemmatize(word) for word in text] \n",
    "    return ' '.join(text)\n",
    "\n",
    "final_dataset['cleaned']=final_dataset['Transcribe'].apply(normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extracting key phrases and topics\n",
    "([Go to top](#Capstone-8:-Bringing-It-All-Together))\n",
    "\n",
    "Use this section to extract the key phrases and topics from the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: keybert in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: rich>=10.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from keybert) (13.3.4)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from keybert) (1.2.0)\n",
      "Requirement already satisfied: sentence-transformers>=0.3.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from keybert) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from keybert) (1.22.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich>=10.4.0->keybert) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich>=10.4.0->keybert) (2.14.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn>=0.22.2->keybert) (1.10.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn>=0.22.2->keybert) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn>=0.22.2->keybert) (1.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence-transformers>=0.3.8->keybert) (0.13.4)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence-transformers>=0.3.8->keybert) (4.28.1)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence-transformers>=0.3.8->keybert) (0.1.98)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence-transformers>=0.3.8->keybert) (2.0.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence-transformers>=0.3.8->keybert) (0.15.1)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence-transformers>=0.3.8->keybert) (4.64.1)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence-transformers>=0.3.8->keybert) (3.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (4.4.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.28.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (21.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (11.7.4.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (2.0.0)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (11.7.99)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (65.6.3)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (0.38.4)\n",
      "Requirement already satisfied: lit in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (16.0.1)\n",
      "Requirement already satisfied: cmake in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.26.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (0.13.3)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk->sentence-transformers>=0.3.8->keybert) (8.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (9.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (1.26.8)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "# I have used keybert library to extract keyword from each text files.\n",
    "!pip install keybert\n",
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am using all-mpnet-base-v2 keybert model and then extracting top 30 keywords from each text.\n",
    "keyword_model = KeyBERT(model='all-mpnet-base-v2')\n",
    "def keyword_extractor_keybert(text):\n",
    "    keywords = keyword_model.extract_keywords(text,keyphrase_ngram_range=(1, 3), stop_words='english',highlight=False,top_n=30)\n",
    "    keywords_list= list(keywords)\n",
    "    return keywords_list\n",
    "final_dataset['keywords']=final_dataset['cleaned'].apply(keyword_extractor_keybert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset['keywords']=final_dataset['keywords'].apply(lambda x: list(dict(x).keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating the dashboard\n",
    "([Go to top](#Capstone-8:-Bringing-It-All-Together))\n",
    "\n",
    "Use this section to create the dashboard for your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer/code here\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "# In this function I am encoding my all keyword extracted from the text and then I am calculating the cosine similarity between the input keywords and the keywords I already have and then sorting on the based of similarity score I am retrieving top 5 video.\n",
    "\n",
    "def rank_videos(input_keywords):\n",
    "    video_encode = model.encode(final_dataset['keywords'].apply(lambda x: ' '.join(x)).tolist())\n",
    "    input_encode = model.encode(' '.join(input_keywords))\n",
    "    similarity_score = cosine_similarity(input_encode.reshape(1, -1), video_encode)\n",
    "    video_rankings = list(enumerate(similarity_score[0]))\n",
    "    video_rankings = sorted(video_rankings, key=lambda x: x[1], reverse=True)\n",
    "    return video_rankings\n",
    "# Generate recommendations\n",
    "def get_videos(input_keywords):\n",
    "    ranked_videos = rank_videos(input_keywords)\n",
    "    indexes = [i[0] for i in ranked_videos]\n",
    "    similarity_score=[i[1] for i in ranked_videos]\n",
    "    return final_dataset.iloc[indexes[:5]]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which videos you want ? or Type Exit or QuitI want video on ml\n",
      "I would like to recommend this 5 videos :\n",
      "1) https://nlp-project-transcribevideos.s3.amazonaws.com/videos/Mod03_Sect03_part3.mp4\n",
      "2) https://nlp-project-transcribevideos.s3.amazonaws.com/videos/Mod06_Intro.mp4\n",
      "3) https://nlp-project-transcribevideos.s3.amazonaws.com/videos/Mod04_Sect02_part1.mp4\n",
      "4) https://nlp-project-transcribevideos.s3.amazonaws.com/videos/Mod03_Sect07_part2.mp4\n",
      "5) https://nlp-project-transcribevideos.s3.amazonaws.com/videos/Mod05_Sect01_ver2.mp4\n",
      "Which videos you want ? or Type Exit or QuitI want video on nlp\n",
      "I would like to recommend this 5 videos :\n",
      "1) https://nlp-project-transcribevideos.s3.amazonaws.com/videos/Mod06_Intro.mp4\n",
      "2) https://nlp-project-transcribevideos.s3.amazonaws.com/videos/Mod03_Sect03_part3.mp4\n",
      "3) https://nlp-project-transcribevideos.s3.amazonaws.com/videos/Mod06_Sect01.mp4\n",
      "4) https://nlp-project-transcribevideos.s3.amazonaws.com/videos/Mod03_Sect07_part2.mp4\n",
      "5) https://nlp-project-transcribevideos.s3.amazonaws.com/videos/Mod03_Sect02_part3.mp4\n",
      "Which videos you want ? or Type Exit or QuitI want videos on imagenet\n",
      "I would like to recommend this 5 videos :\n",
      "1) https://nlp-project-transcribevideos.s3.amazonaws.com/videos/Mod05_WrapUp_ver2.mp4\n",
      "2) https://nlp-project-transcribevideos.s3.amazonaws.com/videos/Mod03_Sect02_part3.mp4\n",
      "3) https://nlp-project-transcribevideos.s3.amazonaws.com/videos/Mod05_Sect01_ver2.mp4\n",
      "4) https://nlp-project-transcribevideos.s3.amazonaws.com/videos/Mod06_Sect02.mp4\n",
      "5) https://nlp-project-transcribevideos.s3.amazonaws.com/videos/Mod05_Intro.mp4\n",
      "Which videos you want ? or Type Exit or Quitexit\n"
     ]
    }
   ],
   "source": [
    "while True: \n",
    "    keyword= input(\"Which videos you want ? or Type Exit or Quit \")\n",
    "    if keyword==\"Exit\" or keyword==\"exit\" or keyword==\"quit\" or keyword==\"Quit\":\n",
    "        break\n",
    "    keyword=keyword_extractor_keybert(keyword)\n",
    "    keys=lambda x: list(dict(x).keys())\n",
    "    keyword=keys(keyword)\n",
    "    result=get_videos(list(keyword))\n",
    "    print(\"I would like to recommend this 5 videos :\")\n",
    "    for index,i in enumerate(result):\n",
    "        index=index+1\n",
    "        print(f'{index}) https://nlp-project-transcribevideos.s3.amazonaws.com/videos/{i}'+'.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b71a13339a0be9489ff337af97259fe0ed71e682663adc836bae31ac651d564e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
